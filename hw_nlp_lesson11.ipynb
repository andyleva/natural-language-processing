{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КУРС \"Введение в обработку естественного языка\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ДЗ к Уроку 11. Модель Transformer-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобраться с моделькой перевода (с механизмом внимания) как она устроена, запустить для перевода с русского на английский (при желании можно взять другие пары языков)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (2.25.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (0.11.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (4.59.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (1.20.1)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (3.19.1)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers[sentencepiece]) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers[sentencepiece]) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers[sentencepiece]) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers[sentencepiece]) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers[sentencepiece]) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers[sentencepiece]) (2.10)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers \n",
    "Попробуем трансформеры в машинном переводе.\n",
    "Возьмем самый простой пример, с готовой обученной моделью. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7414bc1adb84e3fb2db41ade570bd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b3ec37f5d644d695cd16750fa0356e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc71574e3086494bbf999fb9458772bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5255320b69124a448437719c98f09f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Hallo Welt, das ist Roboter, und es wäre zu zeigen, dass es richtig tun!'}]\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import pipeline\n",
    "translator = pipeline('translation_en_to_de')\n",
    "text = 'Hello world! This is robot, and it would be show that do it correct!'\n",
    "translation = translator(text)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако для русского языка простой pipeline не сработает.\n",
    "Для этой задачи надо сделать кастомный pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62c98c640a14aa2890dd7d85a2f785b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9515801f474e309084ea26fd370803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf1f375f03e40e1af81b0f7e31a56a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c9b2fbf4b147a4b21069831c36356f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/803k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bcc5cd40af4123ab42674b6a75fb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1a7760a54b467280306279bca7a3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/307M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-ru-en.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ru-en')\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3704: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\transformers\\generation\\tf_utils.py:1800: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's me and I going to show you how to do it right!\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = 'Привет! Это я и я собираюсь показать вам как делать это правильно!'\n",
    "tokenized_text = tokenizer.prepare_seq2seq_batch([text])\n",
    "translation = model.generate(**tokenized_text)\n",
    "translation_text = tokenizer.batch_decode(translation, skip_special_tokens=True)[0]\n",
    "print(translation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dostoevsky = \"\"\"\n",
    "                В конце ноября, в оттепель, часов в девять утра, поезд Петербургско-Варшавской железной дороги на всех парах подходил к Петербургу. Было так сыро и туманно, что насилу рассвело; в десяти шагах, вправо и влево от дороги, трудно было разглядеть хоть что-нибудь из окон вагона. Из пассажиров были и возвращавшиеся из-за границы; но более были наполнены отделения для третьего класса, и все людом мелким и деловым, не из очень далека. Все, как водится, устали, у всех отяжелели за ночь глаза, все назяблись, все лица были бледножелтые, под цвет тумана.\n",
    "\n",
    "В одном из вагонов третьего класса, с рассвета, очутились друг против друга, у самого окна, два пассажира, - оба люди молодые, оба почти налегке, оба не щегольски одетые, оба с довольно замечательными физиономиями, и оба пожелавшие, наконец, войти друг с другом в разговор. Если б они оба знали один про другого, чем они особенно в эту минуту замечательны, то, конечно, подивились бы, что случай так странно посадил их друг против друга в третьеклассном вагоне петербургско-варшавского поезда. Один из них был небольшого роста, лет двадцати семи, курчавый и почти черноволосый, с серыми, маленькими, но огненными глазами. Нос его был широки сплюснут, лицо скулистое; тонкие губы беспрерывно складывались в какую-то наглую, насмешливую и даже злую улыбку; но лоб его был высок и хорошо сформирован и скрашивал неблагородно развитую нижнюю часть лица. Особенно приметна была в этом лице его мертвая бледность, придававшая всей физиономии молодого человека изможденный вид, несмотря на довольно крепкое сложение, и вместе с тем что-то страстное, до страдания, не гармонировавшее с нахальною и грубою улыбкой и с резким, самодовольным его взглядом. Он был тепло одет, в широкий, мерлушечий, черный, крытый тулуп, и за ночь не зяб, тогда как сосед его принужден был вынести на своей издрогшей спине всю сладость сырой, ноябрьской русской ночи, к которой, очевидно, был не приготовлен. На нем был довольно широкий и толстый плащ без рукавов и с огромным капюшоном, точь-в-точь как употребляют часто дорожные, по зимам, где-нибудь далеко за границей, в Швейцарии, или, например, в Северной Италии, не рассчитывая, конечно, при этом и на такие концы по дороге, как от Эйдкунена до Петербурга. Но что годилось и вполне удовлетворяло в Италии, то оказалось не совсем пригодным в России. Обладатель плаща с капюшоном был молодой человек, тоже лет двадцати шести или двадцати семи, роста немного повыше среднего, очень белокур, густоволос, со впалыми щеками и с легонькою, востренькою, почти совершенно белою бородкой. Глаза его были большие, голубые и пристальные; во взгляде их было что-то тихое, но тяжелое, что-то полное того странного выражения, по которому некоторые угадывают с первого взгляда в субъекте падучую болезнь. Лицо молодого человека было, впрочем, приятное, тонкое и сухое, но бесцветное, а теперь даже до-синя иззябшее. В руках его болтался тощий узелок из старого, полинялого фуляра, заключавший, кажется, все его дорожное достояние. На ногах его были толстоподошвенные башмаки с штиблетами, - все не по-русски. Черноволосый сосед в крытом тулупе все это разглядел, частию от нечего делать, и, наконец, спросил с тою неделикатною усмешкой, в которой так бесцеремонно и небрежно выражается иногда людское удовольствие при неудачах ближнего:\n",
    "\n",
    "- Зябко?\n",
    "\n",
    "И повел плечами.\n",
    "\n",
    "- Очень, - ответил сосед с чрезвычайною готовностью, - и заметьте, это еще оттепель. Что ж, если бы мороз? Я даже не думал, что у нас так холодно. Отвык.\n",
    "\n",
    "- Из-за границы что ль?\n",
    "\n",
    "- Да, из Швейцарии.\n",
    "\n",
    "- Фью! Эк ведь вас!..\n",
    "\n",
    "Черноволосый присвистнул и захохотал.\n",
    "\n",
    "Завязался разговор. Готовность белокурого молодого человека в швейцарском плаще отвечать на все вопросы своего черномазого соседа была удивительная и без всякого подозрения совершенной небрежности, неуместности и праздности иных вопросов. Отвечая, он объявил, между прочим, что действительно долго не был в России, слишком четыре года, что отправлен был за границу по болезни, по какой-то странной нервной болезни, в роде падучей или Виттовой пляски, каких-то дрожаний и судорог. Слушая его, черномазый несколько раз усмехался; особенно засмеялся он, когда на вопрос: \"что же, вылечили?\" - белокурый отвечал, что \"нет, не вылечили\".\n",
    "\n",
    "- Хе! Денег что, должно быть, даром переплатили, а мы-то им здесь верим, - язвительно заметил черномазый.\n",
    "\n",
    "- Истинная правда! - ввязался в разговор один сидевший рядом и дурно одетый господин, нечто в роде закорузлого в подьячестве чиновника, лет сорока, сильного сложения, с красным носом и угреватым лицом: - истинная правда-с, только все русские силы даром к себе переводят!\n",
    "\n",
    "- О, как вы в моем случае ошибаетесь, - подхватил швейцарский пациент, тихим и примиряющим голосом; - конечно, я спорить не могу, потому что всего не знаю, но мой доктор мне из своих последних еще на дорогу сюда дал, да два почти года там на свой счет содержал.\n",
    "\n",
    "- Что ж, некому платить что ли было? - спросил черномазый.\n",
    "\n",
    "- Да, господин Павлищев, который меня там содержал, два года назад помер; я писал потом сюда генеральше Епанчиной, моей дальней родственнице, но ответа не получил. Так с тем и приехал.\n",
    "\n",
    "- Куда же приехали-то?\n",
    "\n",
    "- То-есть, где остановлюсь?.. Да не знаю еще, право… так…\n",
    "\n",
    "- Не решились еще?\n",
    "\n",
    "И оба слушателя снова захохотали.\n",
    "\n",
    "- И небось в этом узелке вся ваша суть заключается? - спросил черномазый.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the end of November, in the throes, at nine o'clock in the morning, the Petersburg-Warshav railway train on all the pairs approached Petersburg. It was so damp and foggy that his eyes were so thin; ten feet to the right and to the left of the road, it was hard to see anything out of the windows of the wagon grudgingly. Out of the passengers came back from abroad; but they were filled with branches for the third class, and all men were very small and business, not very far apart. Everything was out of the blue, tired, all over the night, all eyes were heavy, all the faces were pale, all the faces were pale, were in the colour of the fog. In one of the third class wagons, out of the sun, came out against each other, out of the height of the window, two passengers, both of them were young, both of them were easy, both of them were not lightly dressed, both of them were rather stunned in the night, all of them, all of them were pale, all of them were pale, finally in the colours, in one of them, out of the other countries, out of the sun, out of the passengers, out of us, out of the two passengers, out of the two passengers, both were both of the young, both, both of the young, both of the light, both was light, both was a man, both light, both light, both, both was light, both, both, both of the man, both was light, both, both, both was light, both, both, both were not light, both, both of the same in the same in the same in the same in one in the same in the same in the same in the same in the same in the same in the same in the same in the same in the same in the same in the other.\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = dostoevsky\n",
    "tokenized_text = tokenizer.prepare_seq2seq_batch([text])\n",
    "translation = model.generate(**tokenized_text)\n",
    "translation_text = tokenizer.batch_decode(translation, skip_special_tokens=True)[0]\n",
    "print(translation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
